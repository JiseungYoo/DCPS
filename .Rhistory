library('utils')
## Huber loss
loss_huber <- function(f, y, delta=1){
ifelse(abs(y-f) <= delta,
0.5*(y-f)^2, delta*(abs(y-f) - 0.5*delta))
}
## squared error loss
loss_square <- function(f, y) {
(y-f)^2
}
## absolute error loss
loss_absolute <- function(f, y) {
abs(y-f)
}
## tilted absolute error loss
## tau - target quantile for prediction
loss_tilted <- function(f, y, tau=0.75) {
ifelse(y-f > 0, (y-f) * tau, (y-f) * (tau - 1))
}
## plot loss as functions of residual (y-f)
curve(loss_square(0, x), from=-2, to=2,
xlab='y - f', ylab='loss')
curve(loss_absolute(0, x), from=-2, to=2, add=T, col=2)
curve(loss_tilted(0, x, 0.75), from=-2, to=2, add=T, col=3)
curve(loss_huber(0, x), from=-2, to=2, add=T, col=4)
legend('top', c('squared','absolute','tilted 0.75','Huber'),
col=1:4, lty=1, bty='n')
## constant prediction for given loss
## this applies decision theory predict the
## value 'f' that minimizes the sum of loss
## for loss=loss_square, this returns mean(y)
## for loss=loss_absolute, this returns quantile(y, probs=0.5)
## for loss=loss_huber, this returns some other value
const_pred <- function(y, loss=loss_huber,
limits=c(-1e10,1e10), ...) {
sum_loss <- function(f) sum(loss(f, y, ...))
optimize(sum_loss, interval=limits)$minimum
}
## constant prediction for given loss
## this applies decision theory predict the
## value 'f' that minimizes the sum of loss
## for loss=loss_square, this returns mean(y)
## for loss=loss_absolute, this returns quantile(y, probs=0.5)
## for loss=loss_huber, this returns some other value
const_pred <- function(y, loss=loss_huber, limits=c(-1e10,1e10), ...) {
sum_loss <- function(f) sum(loss(f, y, ...))
optimize(sum_loss, interval=limits)$minimum
}
## const_pred examples
y1 <- rexp(1000) ## mean = 1.000, median = 0.693
mean(y1)
const_pred(y1, loss=loss_square)
median(y1)
const_pred(y1, loss=loss_absolute)
const_pred(y1, loss=loss_huber)
## fit a stump (using squared error loss: method='anova')
stump <- function(dat, maxdepth=1) {
rpart(y~x, data=dat, method='anova',
minsplit=2,minbucket=1,maxdepth=maxdepth,
cp=0,maxcompete=0,maxsurrogate=0,
usesurrogate=0,xval=0) %>%
## convert to constparty to make easier to
## manipulate predictions from this model
as.constparty
}
## example data from the (built-in) MASS package
y <- MASS::mcycle$accel
x <- MASS::mcycle$times
dat <- data.frame(y=y,x=x)
plot(dat$x, dat$y,
xlab='Time', ylab='Acceleration')
## fit a stump for illustration purposes
fit <- stump(dat)
install.packages("party")
library('magrittr')
library('dplyr')
library('rpart')
library('partykit')
library('utils')
## Huber loss
loss_huber <- function(f, y, delta=1){
ifelse(abs(y-f) <= delta,
0.5*(y-f)^2, delta*(abs(y-f) - 0.5*delta))
}
## squared error loss
loss_square <- function(f, y) {
(y-f)^2
}
## absolute error loss
loss_absolute <- function(f, y) {
abs(y-f)
}
## tilted absolute error loss
## tau - target quantile for prediction
loss_tilted <- function(f, y, tau=0.75) {
ifelse(y-f > 0, (y-f) * tau, (y-f) * (tau - 1))
}
## plot loss as functions of residual (y-f)
curve(loss_square(0, x), from=-2, to=2,
xlab='y - f', ylab='loss')
curve(loss_absolute(0, x), from=-2, to=2, add=T, col=2)
curve(loss_tilted(0, x, 0.75), from=-2, to=2, add=T, col=3)
curve(loss_huber(0, x), from=-2, to=2, add=T, col=4)
legend('top', c('squared','absolute','tilted 0.75','Huber'),
col=1:4, lty=1, bty='n')
## constant prediction for given loss
## this applies decision theory predict the
## value 'f' that minimizes the sum of loss
## for loss=loss_square, this returns mean(y)
## for loss=loss_absolute, this returns quantile(y, probs=0.5)
## for loss=loss_huber, this returns some other value
const_pred <- function(y, loss=loss_huber, limits=c(-1e10,1e10), ...) {
sum_loss <- function(f) sum(loss(f, y, ...))
optimize(sum_loss, interval=limits)$minimum
}
## const_pred examples
y1 <- rexp(1000) ## mean = 1.000, median = 0.693
mean(y1)
const_pred(y1, loss=loss_square)
median(y1)
const_pred(y1, loss=loss_absolute)
const_pred(y1, loss=loss_huber)
## fit a stump (using squared error loss: method='anova')
stump <- function(dat, maxdepth=1) {
rpart(y~x, data=dat, method='anova',
minsplit=2,minbucket=1,maxdepth=maxdepth,
cp=0,maxcompete=0,maxsurrogate=0,
usesurrogate=0,xval=0) %>%
## convert to constparty to make easier to
## manipulate predictions from this model
as.constparty
}
## example data from the (built-in) MASS package
y <- MASS::mcycle$accel
x <- MASS::mcycle$times
dat <- data.frame(y=y,x=x)
plot(dat$x, dat$y,
xlab='Time', ylab='Acceleration')
## fit a stump for illustration purposes
fit <- stump(dat)
## plot mean (squared errr loss) of data in each partition
lines(dat$x, predict(fit))
## plot prediction using Huber loss in each partition
lines(dat$x, predict(fit,
FUN=function(y, w) const_pred(y, loss_huber)),
lty=2)
## adjust data then plot mean of adjusted data in each partition
fit$fitted$`(response)` <- fit$fitted$`(response)` - 50
lines(dat$x, predict(fit), lty=3)
## add a legend
legend('topleft',
c('mean', 'Huber loss', 'mean - 50'),
lty=c(1,2,3), bty='n')
## initial model that makes same prediction
## regardless of features
init_pred <- function(dat, loss=loss_huber) {
obj <- list(f = const_pred(dat$y, loss), dat=dat)
class(obj) <- c('init_pred', class(obj))
return(obj)
}
## initial model predictions
## define a 'predict' function for the initial model
predict.init_pred <- function(obj, newdata, ...) {
if(missing(newdata))
return(rep(obj$f, nrow(obj$dat)))
return(rep(obj$f, nrow(newdata)))
}
## finite difference gradients (used to compute gradient of avg loss)
fdGrad <- function (pars, fun, ...,
.relStep = (.Machine$double.eps)^(1/2),
minAbsPar = 0) {
npar <- length(pars)
incr <- ifelse(abs(pars) <= minAbsPar, .relStep,
(abs(pars)-minAbsPar) * .relStep)
sapply(1:npar, function(i) {
del <- rep(0,npar)
del[i] <- incr[i]
(do.call(fun, list(pars+del, ...)) -
do.call(fun, list(pars-del, ...)))/incr[i]/2
})
}
## gradient boosting algorithm
## follows HTF Alg. 10.3
## dat - data frame with 'y' and 'x'
## M   - number of committee members
## fit - function to fit weak learner
## loss - loss function(f, y)
## rho  - learning rate; should be in (0,1]
gradient_boost <- function(dat, M=10, fit=stump, loss = loss_huber,
rho=0.25, progress=TRUE, ...) {
## list to store committee member information
fits <- list()
## step 1.
## fit initial model (constant prediction)
fits[[1]] <- init_pred(dat, loss)
## comptue initial predictions
f <- predict(fits[[1]])
##initialize progress bar
if(progress)
pb <- txtProgressBar(min=1, max=M, initial=2, style=3)
## step 2.
## add committee members
for(i in 2:M) {
## step 2.a.
## compute gradient of sum loss w/respect to predictions
r <- -rho*fdGrad(f, function(f0) sum(loss(f0, dat$y)))
## step 2.b.
## fit a tree to gradient values to get tree structure
s <- stump(dat %>% mutate(y=r))
## step 2.c.
## change the '(response)' element of the 'fitted' slot
## such that when we make predictions from this committee
## member, they're based on the residual from previous
## iteration; this process would look different for
## classification problems, or for regression loss
## functions that are not based on this type of residual
s$fitted$`(response)` <- y-f
## step 2.d.
## update predictions using new committee member
f <- f + predict(s,
FUN=function(y,w) const_pred(y, loss))
## add committee member to list
fits[[i]] <- s
## update progress bar
if(progress)
setTxtProgressBar(pb, value = i)
}
## close progress bar
if(progress)
close(pb)
return(fits)
}
## do gradient boosting to 1000 iterations with Huber loss
fits_huber <- gradient_boost(dat, M=1000, loss=loss_huber)
## plot how number of trees affects fit
manipulate({
## plot acceleration data
plot(dat$x, dat$y,
xlab='Time', ylab='Acceleration')
legend('topleft', legend=paste0('M = ', m_sl), bty='n')
## compute predictions using 'm_sl' committee members
x_plot <- seq(0, 60, 0.1)
f <- rowSums(sapply(fits_huber[1:m_sl], function(fit)
predict(fit, data.frame(x=x_plot),
FUN=function(y,w) const_pred(y, loss_huber))))
## plot predictions
lines(x_plot, f)
}, m_sl = slider(1, 1000, 1, 'M', 1))
## do gradient boosting to 1000 iterations with Huber loss
fits_tilted <- gradient_boost(dat, M=1000, loss=loss_tilted)
## plot how number of trees affects fit
manipulate({
## plot acceleration data
plot(dat$x, dat$y,
xlab='Time', ylab='Acceleration')
legend('topleft', legend=paste0('M = ', m_sl), bty='n')
## compute predictions using 'm_sl' committee members
x_plot <- seq(0, 60, 0.1)
f <- rowSums(sapply(fits_tilted[1:m_sl], function(fit)
predict(fit, data.frame(x=x_plot),
FUN=function(y,w) const_pred(y, loss_tilted))))
## plot predictions
lines(x_plot, f)
}, m_sl = slider(1, 1000, 1, 'M', 1))
library('qrnn') ## for the 'tilted.approx' function
loss_tilted_huber <- function(f, y, tau=0.75, eps=1)
tilted.approx(y-f, tau, eps)
loss_tilted_huber <- function(f, y, tau=0.75, eps=1){
tilted.approx(y-f, tau, eps)
}
## plot the tilted huber loss versus tilted loss
## in both cases predictions that are too small are worse
curve(loss_tilted_huber(0, x), from=-2, to=2,
xlab='y-f', ylab='loss', lty=2)
curve(loss_tilted(0, x, 0.75), from=-2, to=2, add=T, lty=1)
legend('top', c('tilted 0.75','tilted Huber 0.75'),
lty=1:2, bty='n')
## do gradient boosting to 1000 iterations with tilted Huber loss
fits_tilted_huber <-
gradient_boost(dat, M=1000, loss=loss_tilted_huber)
## plot how number of trees affects fit
manipulate({
## plot acceleration data
plot(dat$x, dat$y,
xlab='Time', ylab='Acceleration')
legend('topleft', legend=paste0('M = ', m_sl), bty='n')
## compute predictions using 'm_sl' committee members
x_plot <- seq(0, 60, 0.1)
f <- rowSums(sapply(fits_tilted_huber[1:m_sl], function(fit)
predict(fit, data.frame(x=x_plot),
FUN=function(y,w) const_pred(y, loss_tilted_huber))))
## plot predictions
lines(x_plot, f)
}, m_sl = slider(1, 1000, 1, 'M', 1))
## plot how number of trees affects fit
manipulate({
## plot acceleration data
plot(dat$x, dat$y,
xlab='Time', ylab='Acceleration')
legend('topleft', legend=paste0('M = ', m_sl), bty='n')
## compute predictions using 'm_sl' committee members
x_plot <- seq(0, 60, 0.1)
f <- rowSums(sapply(fits_tilted_huber[1:m_sl], function(fit)
predict(fit, data.frame(x=x_plot),
FUN=function(y,w) const_pred(y, loss_tilted_huber))))
## plot predictions
lines(x_plot, f)
}, m_sl = slider(1, 1000, 1, 'M', 1))
## plot the tilted huber loss versus tilted loss
## in both cases predictions that are too small are worse
curve(loss_tilted_huber(0, x), from=-2, to=2,
xlab='y-f', ylab='loss', lty=2)
loss_tilted_huber <- function(f, y, tau=0.75, eps=1){
tilted.approx(y-f, tau, eps)
}
## plot the tilted huber loss versus tilted loss
## in both cases predictions that are too small are worse
curve(loss_tilted_huber(0, x), from=-2, to=2,
xlab='y-f', ylab='loss', lty=2)
install.packages("qrnn")
library('qrnn') ## for the 'tilted.approx' function
loss_tilted_huber <- function(f, y, tau=0.75, eps=1){
tilted.approx(y-f, tau, eps)
}
## plot the tilted huber loss versus tilted loss
## in both cases predictions that are too small are worse
curve(loss_tilted_huber(0, x), from=-2, to=2,
xlab='y-f', ylab='loss', lty=2)
curve(loss_tilted(0, x, 0.75), from=-2, to=2, add=T, lty=1)
legend('top', c('tilted 0.75','tilted Huber 0.75'),
lty=1:2, bty='n')
## do gradient boosting to 1000 iterations with tilted Huber loss
fits_tilted_huber <-
gradient_boost(dat, M=1000, loss=loss_tilted_huber)
## plot how number of trees affects fit
manipulate({
## plot acceleration data
plot(dat$x, dat$y,
xlab='Time', ylab='Acceleration')
legend('topleft', legend=paste0('M = ', m_sl), bty='n')
## compute predictions using 'm_sl' committee members
x_plot <- seq(0, 60, 0.1)
f <- rowSums(sapply(fits_tilted_huber[1:m_sl], function(fit)
predict(fit, data.frame(x=x_plot),
FUN=function(y,w) const_pred(y, loss_tilted_huber))))
## plot predictions
lines(x_plot, f)
}, m_sl = slider(1, 1000, 1, 'M', 1))
gc
gc()
gc()
knitr::opts_chunk$set(echo = TRUE)
working_directory <- file.path("C:", "Users", "Max", "Documents", "GitHub",
"DCPS")
# working_directory <- file.path("/Users/jiseungyoo/Desktop/DCPS/data")
box_directory <- file.path(working_directory, "..", "..", "..",
"Box", "DCPS-SERP Data Internal 2019")
setwd(working_directory)
#getwd()
## choose required packages ----
c("tidyverse", "magrittr", "rlang", "glue",
"haven", "labelled", "writexl",
"gt", "gtsummary", "webshot2", 'readxl',
"scales", "grid", "ggtext", "patchwork"
)  |>
purrr::walk(~require(.x, character.only = TRUE))
excel_file <- file.path( box_directory, "Data", "Raw",
"Student Data through SY22-23 Updated.xlsx")
demog_student.year <-
read_excel(excel_file, sheet = "Demographic")
enrollment_student.school.year <-
read_excel(excel_file, sheet = "Enrollment and Attendance")
courses_student.school.year <-
map(map_vec(c(19:22), ~glue("Courses SY{.x}-{.x+1}")),
~excel_file %>% read_excel(sheet = .x)) %>%
list_rbind()
tests_student.year <-
read_excel(excel_file, sheet = "PARCC")
assessments_student.year <-
read_excel(excel_file, sheet = "Formative Assessments") %>%
filter(Assessment_Type %in% c("DIBELS", "RI", "TRC"))
## Determine Primary Teacher ----
mode_stat <- function(x, na.rm = TRUE) {
if(na.rm) x = x[!is.na(x)]
ux <- unique(x)
return(ux[which.max(tabulate(match(x, ux)))])
}
primary_teacher <-
courses_student.school.year %>%
group_by(StudentID, Grade, School, courses_sy_start) %>%
mutate(Employee_Number = ifelse(any(Subject_Code == "EE"),
mode_stat(Employee_Number[Subject_Code == "EE"]), Employee_Number)) %>%
reframe(main_teacher = mode_stat(Employee_Number)) %>%
ungroup() %>%
mutate(Grade = ifelse(str_trim(Grade) == "K", "0", Grade) %>% parse_double()) %>%
rename(year = courses_sy_start, school_id = School)
## clean test scores ----
assessments_student.year %<>%
mutate(etime = case_match(Assessment_Window,
"BOY" ~ "f", "MOY" ~ "m", "EOY" ~ "s", .default = NULL),
tname = ifelse(Assessment_Type == "RI", "sri", Assessment_Type)) %>%
rename(pb = Proficiency_Level, ss = Proficiency_Score) %>%
select(-Assessment_Window, -Assessment_Type) %>%
pivot_wider(names_from = c(tname, etime),
values_from = c(pb, ss), names_glue = "s_{tname}_{.value}_{etime}") %>%
select(-contains("TRC_ss")) %>%
mutate(across(contains("_pb_"), ~case_match(.x,
c("Below Basic", "Well Below Benchmark", "Far Below Proficient") ~ 1,
c("Basic", "Below Benchmark", "Below Proficient") ~ 2,
c("Proficient", "Benchmark", "At Benchmark") ~ 3,
c("Advanced", "Above Benchmark", "Above Proficient") ~ 4,
.default = NULL)),
across(contains("_ss_"), ~parse_integer(.x)))
## merge and reformat ----
df <- enrollment_student.school.year %>%
group_by(StudentID, SchoolYearStart) %>%
filter(Membership_Days==max(Membership_Days)) %>%
reframe(Grade = min(ifelse(str_trim(Grade) == "K", "0", Grade) %>% parse_double(), na.rm = TRUE),
school_id = min(School_ID, na.rm = TRUE),
s_days_enrolled = max(Membership_Days)) %>%
rename(year = SchoolYearStart) %>%
left_join(
tests_student.year %>% rename(Grade = student_Grade),
join_by(StudentID, Grade), relationship = "many-to-many") %>%
left_join(
primary_teacher, join_by(StudentID, Grade, year, school_id), relationship = "many-to-many"
) %>%
left_join(demog_student.year %>%
mutate(Grade = ifelse(str_trim(Grade) == "K", "0", Grade) %>% parse_double()) %>%
rename(year = School_Year_Start), join_by(StudentID, Grade, year)) %>%
left_join(assessments_student.year %>%
rename(year = School_Year_Start), join_by(StudentID, Grade, year)) %>%
mutate(s_race = case_match(Race_Ethnicity,
"American Indian or Alaska Native" ~ 1,
"Asian" ~ 2,
"Black or African American" ~ 3,
"Hispanic/Latino" ~ 4,
"Two or More Races" ~ 5,
"White" ~ 6, .default = NULL),
s_female = case_match(Gender, "F" ~ 1, "M" ~ 0, .default = NULL),
s_sped = case_match(SPED, "Y" ~ 1, "N" ~ 0, .default = NULL),
s_lep = case_match(EL_Indicator, "Yes" ~ 1, "No" ~ 0, .default = NULL),
ela_test_grade = case_when(str_starts(ela_testcode, "ELA")
~ parse_number(str_remove(math_testcode, "ELA")), .default = NULL),
math_test_grade = case_when(str_starts(ela_testcode, "MAT")
~ parse_number(str_remove(ela_testcode, "MAT")), .default = NULL),
employeeid = parse_number(main_teacher)) %>%
rename(
usi = StudentID,
schoolyear_fall = year,
s_grade = Grade,
schoolid1 = school_id,
s_parcc_pb_ela = ela_perf_level,
s_parcc_ss_ela = ela_scale_score,
s_parcc_pb_math = math_perf_level,
s_parcc_ss_math = math_scale_score,
birthdate = Birth_Date
) %>%
select(usi, schoolyear_fall, s_grade, schoolid1, ela_test_grade,
s_parcc_pb_ela, s_parcc_ss_ela, math_test_grade, s_parcc_pb_math,
s_parcc_ss_math, employeeid, s_female, s_race,
birthdate, s_lep, s_sped, s_days_enrolled, s_DIBELS_pb_s,
s_DIBELS_pb_f, s_DIBELS_pb_m, s_TRC_pb_s, s_TRC_pb_f, s_TRC_pb_m,
s_sri_pb_s, s_sri_pb_m, s_sri_pb_f, s_DIBELS_ss_s, s_DIBELS_ss_f,
s_DIBELS_ss_m, s_sri_ss_s, s_sri_ss_m, s_sri_ss_f)
names(df) <- str_to_lower(names(df))
df_max <- df
df_js <-
courses_student.school.year %>%
group_by(StudentID, Grade, School, courses_sy_start) %>%
filter(grepl("Elementary|language|english|reading|writing|literacy", Title, ignore.case = TRUE))%>%
filter(!grepl("computer|spanish|native|latin|arabic|financial", Title, ignore.case = TRUE))  %>%
reframe(lan_teacher = mode_stat(Employee_Number)) %>%
ungroup() %>%
mutate(Grade = ifelse(str_trim(Grade) == "K", "0", Grade) %>% parse_double()) %>%
rename(
schoolyear_fall = courses_sy_start,
schoolid1 = School,
usi = StudentID,
s_grade = Grade,
language_teacher= lan_teacher) %>%
right_join(
df_max, join_by(usi, s_grade, schoolyear_fall, schoolid1)) %>%
group_by(schoolyear_fall, s_grade) %>%
mutate(across(
c(s_dibels_ss_s, s_dibels_ss_f, s_dibels_ss_m,
s_sri_ss_s, s_sri_ss_m, s_sri_ss_f),
~ scale(., center = TRUE, scale = TRUE),
.names = "{.col}_sd"
)) %>%
ungroup() %>%
rename(
s_dibels_ss_sd_s= s_dibels_ss_s_sd,
s_dibels_ss_sd_f= s_dibels_ss_f_sd,
s_dibels_ss_sd_m= s_dibels_ss_m_sd,
s_sri_ss_sd_s= s_sri_ss_s_sd,
s_sri_ss_sd_m= s_sri_ss_m_sd,
s_sri_ss_sd_f= s_sri_ss_f_sd,
teacherid = employeeid,
employeeid = language_teacher
) %>% ### switch teacher main - language
mutate(
s_white = as.integer(s_race == "6"),
s_black = as.integer(s_race == "3"),
s_native_american = as.integer(s_race == "1"),
s_asian = as.integer(s_race == "2"),
s_latinx = as.integer(s_race == "4"),
s_race_multi_other = as.integer(s_race == "5"),
employeeid = as.numeric(employeeid))
# df_js_sd %>%
#   filter(s_grade %in% c(0:8)) %>%
#   write_dta( file.path(working_directory, "student_level_js.dta"))
